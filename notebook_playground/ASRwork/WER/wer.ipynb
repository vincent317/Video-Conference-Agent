{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webvtt\n",
    "import jiwer\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workflow: transcribe result -> txt (jiwer transformation) -> difftxt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws = webvtt.read('aws-transcribe-result.vtt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_transcribe = []\n",
    "for caption in aws:\n",
    "    aws_transcribe.append(caption.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/wb29rdrn08q57rfbksgz4z9h0000gn/T/ipykernel_1050/3453676625.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  sample_transcript=pd.read_csv('sample-transcript.txt', sep=\"/n\", header=None)[0].tolist()\n"
     ]
    }
   ],
   "source": [
    "sample_transcript=pd.read_csv('sample-transcript.txt', sep=\"/n\", header=None)[0].tolist() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Speech-to-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_transcribe = []\n",
    "f = open('gcp-transcribe-result.json')\n",
    "gcp = json.load(f)\n",
    "for i in gcp['results']:\n",
    "    gcp_transcribe.append(i[\"alternatives\"][0][\"transcript\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/wb29rdrn08q57rfbksgz4z9h0000gn/T/ipykernel_1050/1754411159.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  gcp_transcribe=pd.read_csv('gcp-transcribe-result.txt', sep=\"/n\", header=None)[0].tolist()\n"
     ]
    }
   ],
   "source": [
    "gcp_transcribe=pd.read_csv('gcp-transcribe-result.txt', sep=\"/n\", header=None)[0].tolist() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Speech-to-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/wb29rdrn08q57rfbksgz4z9h0000gn/T/ipykernel_1050/3963438028.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  azure_transcribe=pd.read_csv('azure-transcribe-result.txt', sep=\"/n\",header=None)[0].tolist()\n"
     ]
    }
   ],
   "source": [
    "azure_transcribe=pd.read_csv('azure-transcribe-result.txt', sep=\"/n\",header=None)[0].tolist() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoom Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/wb29rdrn08q57rfbksgz4z9h0000gn/T/ipykernel_1050/1613688721.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  zoom_transcribe=pd.read_csv('zoom-transcribe.txt', sep=\"/n\",header=None)[0].tolist()\n"
     ]
    }
   ],
   "source": [
    "zoom_transcribe=pd.read_csv('zoom-transcribe.txt', sep=\"/n\",header=None)[0].tolist() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/wb29rdrn08q57rfbksgz4z9h0000gn/T/ipykernel_1050/940662847.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  yt_transcribe=pd.read_csv('yt-transcribe-result.txt', sep=\"/n\",header=None)[0].tolist()\n"
     ]
    }
   ],
   "source": [
    "yt_transcribe=pd.read_csv('yt-transcribe-result.txt', sep=\"/n\",header=None)[0].tolist() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.ReduceToSingleSentence(),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.Strip(),\n",
    "    jiwer.ExpandCommonEnglishContractions(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    jiwer.ReduceToListOfListOfWords(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12437185929648241"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jiwer import wer\n",
    "aws_error = wer(sample_transcript, aws_transcribe, truth_transform=transformation, hypothesis_transform=transformation)\n",
    "aws_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19120603015075377"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcp_error = wer(sample_transcript, gcp_transcribe, truth_transform=transformation, hypothesis_transform=transformation)\n",
    "gcp_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11758793969849246"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_error = wer(sample_transcript, azure_transcribe, truth_transform=transformation, hypothesis_transform=transformation)\n",
    "azure_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23492462311557788"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zoom_error = wer(sample_transcript, zoom_transcribe, truth_transform=transformation, hypothesis_transform=transformation)\n",
    "zoom_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14346733668341707"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_error = wer(sample_transcript, yt_transcribe, truth_transform=transformation, hypothesis_transform=transformation)\n",
    "yt_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription list to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# #Reference\n",
    "sample_lst = sum(transformation(sample_transcript), [])\n",
    "with open(\"/Users/yooni/Desktop/vca/ref-txt.txt\",'w') as f:\n",
    "    for cap in sample_lst:\n",
    "        f.write(\"%s\\n\" % cap)\n",
    "        f.write(\" \")\n",
    "    print('Done')\n",
    "\n",
    "# Aws\n",
    "aws_lst = sum(transformation(aws_transcribe), [])\n",
    "with open(\"/Users/yooni/Desktop/vca/aws-txt.txt\",'w') as f:\n",
    "    for cap in aws_lst:\n",
    "        f.write(\"%s\\n\" % cap)\n",
    "        f.write(\" \")\n",
    "    print('Done')\n",
    "\n",
    "# # GCP\n",
    "gcp_lst = sum(transformation(gcp_transcribe), [])\n",
    "with open(\"/Users/yooni/Desktop/vca/gcp-txt.txt\",'w') as f:\n",
    "    for cap in gcp_lst:\n",
    "        f.write(\"%s\\n\" % cap)\n",
    "        f.write(\" \")\n",
    "    print('Done')\n",
    "\n",
    "# #Azure\n",
    "azure_lst = sum(transformation(azure_transcribe), [])\n",
    "with open(\"/Users/yooni/Desktop/vca/azure-txt.txt\",'w') as f:\n",
    "    for cap in azure_lst:\n",
    "        f.write(\"%s\\n\" % cap)\n",
    "        f.write(\" \")\n",
    "    print('Done')\n",
    "\n",
    "# #Zoom\n",
    "zoom_lst = sum(transformation(zoom_transcribe), [])\n",
    "with open(\"/Users/yooni/Desktop/vca/zoom-txt.txt\",'w') as f:\n",
    "    for cap in zoom_lst:\n",
    "        f.write(\"%s\\n\" % cap)\n",
    "        f.write(\" \")\n",
    "    print('Done')\n",
    "\n",
    "# #Youtube\n",
    "yt_lst = sum(transformation(yt_transcribe), [])\n",
    "with open(\"/Users/yooni/Desktop/vca/yt-txt.txt\",'w') as f:\n",
    "    for cap in yt_lst:\n",
    "        f.write(\"%s\\n\" % cap)\n",
    "        f.write(\" \")\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "awsdiff = []\n",
    "with open(\"ref-txt.txt\", 'r') as ref:\n",
    "    with open('aws-txt.txt', 'r') as comp:\n",
    "        diff = difflib.unified_diff(\n",
    "            ref.readlines(),\n",
    "            comp.readlines(),\n",
    "            fromfile='ref',\n",
    "            tofile='comp',\n",
    "        )\n",
    "        for line in diff:\n",
    "            for prefix in ('---', '+++', '@@'):\n",
    "                if line.startswith(prefix):\n",
    "                    break\n",
    "            for prefix in ('- ', '+ '):\n",
    "                if line.startswith(prefix):\n",
    "                    awsdiff.append(line)\n",
    "\n",
    "with open(\"/Users/yooni/Desktop/vca/aws-diff.txt\",'w') as f:\n",
    "    for cap in awsdiff:\n",
    "        f.write(cap)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "gcpdiff = []\n",
    "with open(\"ref-txt.txt\", 'r') as ref:\n",
    "    with open('gcp-txt.txt', 'r') as comp:\n",
    "        diff = difflib.unified_diff(\n",
    "            ref.readlines(),\n",
    "            comp.readlines(),\n",
    "            fromfile='ref',\n",
    "            tofile='comp',\n",
    "        )\n",
    "        for line in diff:\n",
    "            for prefix in ('---', '+++', '@@'):\n",
    "                if line.startswith(prefix):\n",
    "                    break\n",
    "            for prefix in ('- ', '+ '):\n",
    "                if line.startswith(prefix):\n",
    "                    gcpdiff.append(line)\n",
    "\n",
    "with open(\"/Users/yooni/Desktop/vca/gcp-diff.txt\",'w') as f:\n",
    "    for cap in gcpdiff:\n",
    "        f.write(cap)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "azurediff = []\n",
    "with open(\"ref-txt.txt\", 'r') as ref:\n",
    "    with open('azure-txt.txt', 'r') as comp:\n",
    "        diff = difflib.unified_diff(\n",
    "            ref.readlines(),\n",
    "            comp.readlines(),\n",
    "            fromfile='ref',\n",
    "            tofile='comp',\n",
    "        )\n",
    "        for line in diff:\n",
    "            for prefix in ('---', '+++', '@@'):\n",
    "                if line.startswith(prefix):\n",
    "                    break\n",
    "            for prefix in ('- ', '+ '):\n",
    "                if line.startswith(prefix):\n",
    "                    azurediff.append(line)\n",
    "\n",
    "with open(\"/Users/yooni/Desktop/vca/azure-diff.txt\",'w') as f:\n",
    "    for cap in azurediff:\n",
    "        f.write(cap)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "zoomdiff = []\n",
    "with open(\"ref-txt.txt\", 'r') as ref:\n",
    "    with open('zoom-txt.txt', 'r') as comp:\n",
    "        diff = difflib.unified_diff(\n",
    "            ref.readlines(),\n",
    "            comp.readlines(),\n",
    "            fromfile='ref',\n",
    "            tofile='comp',\n",
    "        )\n",
    "        for line in diff:\n",
    "            for prefix in ('---', '+++', '@@'):\n",
    "                if line.startswith(prefix):\n",
    "                    break\n",
    "            for prefix in ('- ', '+ '):\n",
    "                if line.startswith(prefix):\n",
    "                    zoomdiff.append(line)\n",
    "\n",
    "with open(\"/Users/yooni/Desktop/vca/zoom-difftxt\",'w') as f:\n",
    "    for cap in zoomdiff:\n",
    "        f.write(cap)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ytdiff = []\n",
    "with open(\"ref-txt.txt\", 'r') as ref:\n",
    "    with open('yt-txt.txt', 'r') as comp:\n",
    "        diff = difflib.unified_diff(\n",
    "            ref.readlines(),\n",
    "            comp.readlines(),\n",
    "            fromfile='ref',\n",
    "            tofile='comp',\n",
    "        )\n",
    "        for line in diff:\n",
    "            for prefix in ('---', '+++', '@@'):\n",
    "                if line.startswith(prefix):\n",
    "                    break\n",
    "            for prefix in ('- ', '+ '):\n",
    "                if line.startswith(prefix):\n",
    "                    ytdiff.append(line)\n",
    "\n",
    "with open(\"/Users/yooni/Desktop/vca/yt-difftxt\",'w') as f:\n",
    "    for cap in ytdiff:\n",
    "        f.write(cap)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea2fba1330eafecf234206cfb186b9bf7c5755bdd4393c3b282ab3d8c97c006f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
