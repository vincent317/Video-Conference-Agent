from time import time
from datetime import datetime as dt
import os
import time
from pathlib import Path
from . import zoom_service
from azure.storage.blob import BlobServiceClient
from azure.storage.blob import generate_container_sas, ContainerSasPermissions
from azure.storage.blob import generate_blob_sas, BlobSasPermissions
import azure.cognitiveservices.speech as speechsdk
import swagger_client
import requests
import json

# Set up the environmental vairables
storage_account_key = os.environ.get('AUDIO_STORAGE_ACCT_KEY')
storage_account_name = os.environ.get('AUDIO_STORAGE_ACCT_NAME')
connection_string = os.environ.get('CONNECTION_STRING')
container_name = os.environ.get('AUDIO_CONTAINER_NAME')
SUBSCRIPTION_KEY = os.getenv('SPEECH_SUBSCRIPTION_KEY')
SERVICE_REGION = "eastus"
LOCALE = "en-US"

transcription_stroage_account_key = os.getenv('TRANSCRIPTION_STORAGE_ACCT_KEY')
transcription_stroage_account_name = os.getenv('TRANSCRIPTION_STORAGE_ACCT_NAME')
transcription_container_name = os.getenv('TRANSCRIPTION_CONTAINER_NAME')

NAME = "TEST MEETING" # TODO: get from zoom
DESCRIPTION = "N/A" # TODO: get from zoom
PARTICIPANT_COUNT = 3 # TODO: get from zoom

def upload_to_blob_storage(file_path, file_name):
    blob_service_client = BlobServiceClient.from_connection_string(connection_string)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)
    upload_start_time = time.time()
    print("ASR: Uploading to Azure Blob...")
    if not blob_client.exists():
        with open(file_path, "rb") as data:
            blob_client.upload_blob(data)
            print("ASR: Uploading to Azure Blob done, takes %s seconds" % (time.time() - upload_start_time))
    else:
        print("ASR: Blob already exists in Azure Cloud. Skipped uploading.")

def transcribe_from_single_blob(uri, properties):
    """
    Transcribe a single audio file located at `uri` using the settings specified in `properties`
    using the base model for the specified locale.
    """
    transcription_definition = swagger_client.Transcription(
        display_name=NAME,
        description=DESCRIPTION,
        locale=LOCALE,
        content_urls=[uri],
        properties=properties
    )

    return transcription_definition

from datetime import datetime, timedelta
from azure.storage.blob import BlobClient, generate_blob_sas, BlobSasPermissions

def get_blob_sas(storage_account_name, storage_account_key, container_name, blob_name):
    sas_blob = generate_blob_sas(account_name=storage_account_name, 
                                container_name=container_name,
                                blob_name=blob_name,
                                account_key=storage_account_key,
                                permission=BlobSasPermissions(read=True),
                                expiry=datetime.utcnow() + timedelta(hours=1))
    return sas_blob

def get_container_sas(storage_account_name, storage_account_key, container_name):
    container_sas = generate_container_sas(
       account_name=storage_account_name,
       container_name=container_name,
       account_key=storage_account_key,
       permission=ContainerSasPermissions(write=True, read=True, list=True),
       start = datetime.utcnow(),
       expiry=datetime.utcnow() + timedelta(hours=1))
    return container_sas


def _paginate(api, paginated_object):
    """
    The autogenerated client does not support pagination. This function returns a generator over
    all items of the array that the paginated object `paginated_object` is part of.
    """
    yield from paginated_object.values
    typename = type(paginated_object).__name__
    auth_settings = ["api_key"]
    while paginated_object.next_link:
        link = paginated_object.next_link[len(api.api_client.configuration.host):]
        paginated_object, status, headers = api.api_client.call_api(link, "GET",
            response_type=typename, auth_settings=auth_settings)

        if status == 200:
            yield from paginated_object.values
        else:
            raise Exception(f"could not receive paginated data: status {status}")

def transcribe(blob_sas_url, container_sas_url):
    print("ASR: Starting transcription client...")
    # configure API key authorization: subscription_key
    configuration = swagger_client.Configuration()
    configuration.api_key["Ocp-Apim-Subscription-Key"] = SUBSCRIPTION_KEY
    configuration.host = f"https://{SERVICE_REGION}.api.cognitive.microsoft.com/speechtotext/v3.1"

    # create the client object and authenticate
    client = swagger_client.ApiClient(configuration)

    # create an instance of the transcription api class
    api = swagger_client.CustomSpeechTranscriptionsApi(api_client=client)

    properties = swagger_client.TranscriptionProperties()
    properties.display_form_word_level_timestamps_enabled = True
    properties.punctuation_mode = "DictatedAndAutomatic"
    properties.destination_container_url = container_sas_url
    # uncomment the following block to enable and configure speaker separation
    properties.diarization_enabled = True
    properties.diarization = swagger_client.DiarizationProperties(
        swagger_client.DiarizationSpeakersProperties(min_count=1, max_count=PARTICIPANT_COUNT))

    # Use base models for transcription. Comment this block if you are using a custom model.
    transcription_definition = transcribe_from_single_blob(blob_sas_url, properties)

    created_transcription, status, headers = api.transcriptions_create_with_http_info(transcription=transcription_definition)

    # get the transcription Id from the location URI
    transcription_id = headers["location"].split("/")[-1]

    # Log information about the created transcription. If you should ask for support, please
    # include this information.
    print(f"ASR: Created new transcription with id '{transcription_id}' in region {SERVICE_REGION}")
    print("ASR: Checking status.")

    completed = False

    while not completed:
        # wait for 5 seconds before refreshing the transcription status
        time.sleep(15)

        transcription = api.transcriptions_get(transcription_id)
        print(f"ASR: Transcriptions status: {transcription.status}")

        if transcription.status in ("Failed", "Succeeded"):
            completed = True

        if transcription.status == "Succeeded":
            pag_files = api.transcriptions_list_files(transcription_id)
            for file_data in _paginate(api, pag_files):
                if file_data.kind != "Transcription":
                    continue

                results_url = file_data.links.content_url
                results = requests.get(results_url)
                return results
        elif transcription.status == "Failed":
            print(f"ASR: Transcription failed: {transcription.properties.error.message}")

'''
Main entry point for the ASR function.

1. Check if the transcript and duration file already exists. If yes, returns immediately.
2. Get the audio file using Zoom API
3. Use ffmpeg to transfrom the audio file in .wav format.
4. Upload the .wav file to Azure Blob storage.
5. Call Azure REST APIs (swagger_client for connection) to start the ASR job.
    Swagger_client documentation:
    https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/master/samples/batch/python/python-client/main.py
6. Save the transcript & duration file in .txt and .json format respectively.
'''
def asr(meeting_id):
    transcript_file = os.path.join('azure_transcripts', str(meeting_id) + ".txt")
    duration_file = os.path.join('azure_durations', str(meeting_id) + ".json")

    if not os.path.exists(transcript_file) or not os.path.exists(duration_file):
        zoom_m4a_location = zoom_service.get_meeting_audio(meeting_id)
        out_wav = os.path.join("audios", str(meeting_id) + ".wav")
        # Transform to .wav file from m4a file
        if not os.path.exists(out_wav): 
            os.system("ffmpeg -i {0} -acodec pcm_s16le -ac 1 -ar 16000 {1}".format(zoom_m4a_location, out_wav))

        if not os.path.exists(out_wav): 
            raise ValueError("ffmpeg failed. check if that has been installed properly and added to env path.")

        # Upload to Azure Blob Storage
        upload_to_blob_storage(out_wav, out_wav)

        # Get blob signature and url
        blob_name = out_wav
        blob_token = get_blob_sas(storage_account_name, storage_account_key, container_name, blob_name)
        blob_sas_url = 'https://'+storage_account_name+'.blob.core.windows.net/'+container_name+'/'+blob_name+'?'+blob_token
        container_token = get_container_sas(transcription_stroage_account_name, transcription_stroage_account_key, transcription_container_name)
        container_sas_url = 'https://'+transcription_stroage_account_name+'.blob.core.windows.net/'+transcription_container_name+'?'+container_token

        # Start the transcribe job
        results = transcribe(blob_sas_url, container_sas_url)
        content = results.content.decode('utf-8')
        result = json.loads(content)

        # Parse ASR results
        transcription = ''
        duration = {'total':0}
        for segment in result["recognizedPhrases"]:
            transcription += 'Speaker ' + str(segment["speaker"]) + ': ' + segment["nBest"][0]["display"] + '\n\n'
            speaker_duration = segment['durationInTicks'] / 10000000
            if 'Speaker '+ str(segment["speaker"]) not in duration:
                duration['Speaker '+ str(segment["speaker"])] = speaker_duration
            else:
                duration['Speaker '+ str(segment["speaker"])] += speaker_duration
        with open(transcript_file, 'w') as f:
            f.write(transcription)
        with open(duration_file, 'w') as fp:
            json.dump(duration, fp)
        
        # Delete the temp .wav file
        os.remove(out_wav)
    
    print(f"ASR: transcript file saved at {transcript_file} and duration file saved at {duration_file}")
    return transcript_file, duration_file